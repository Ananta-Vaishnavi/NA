{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10568680,"sourceType":"datasetVersion","datasetId":6539858}],"dockerImageVersionId":30408,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update\n!apt-get install -y libgl1-mesa-glx","metadata":{"execution":{"iopub.status.busy":"2025-01-24T08:54:34.570480Z","iopub.execute_input":"2025-01-24T08:54:34.570854Z","iopub.status.idle":"2025-01-24T08:54:43.763635Z","shell.execute_reply.started":"2025-01-24T08:54:34.570780Z","shell.execute_reply":"2025-01-24T08:54:43.762528Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Get:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \nGet:3 https://packages.cloud.google.com/apt google-fast-socket InRelease [1071 B]\nGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nGet:5 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]      \nErr:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease          \n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nHit:6 http://archive.ubuntu.com/ubuntu focal InRelease   \nErr:2 https://packages.cloud.google.com/apt cloud-sdk InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nGet:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\nErr:3 https://packages.cloud.google.com/apt google-fast-socket InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nGet:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1975 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\nGet:10 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1297 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [34.6 kB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1589 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [4276 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4663 kB]\nGet:16 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [4187 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4463 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\nGet:19 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\nFetched 23.0 MB in 3s (7855 kB/s)                           \nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://packages.cloud.google.com/apt gcsfuse-focal InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt cloud-sdk InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt google-fast-socket InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: Failed to fetch http://packages.cloud.google.com/apt/dists/gcsfuse-focal/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: Failed to fetch https://packages.cloud.google.com/apt/dists/cloud-sdk/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: Failed to fetch https://packages.cloud.google.com/apt/dists/google-fast-socket/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nlibgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n0 upgraded, 0 newly installed, 0 to remove and 212 not upgraded.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install deepface\n!pip install plotly\n!pip install -U scikit-learn\n!pip install seaborn\n","metadata":{"execution":{"iopub.status.busy":"2025-01-24T08:54:43.765943Z","iopub.execute_input":"2025-01-24T08:54:43.766322Z","iopub.status.idle":"2025-01-24T08:55:55.975070Z","shell.execute_reply.started":"2025-01-24T08:54:43.766255Z","shell.execute_reply":"2025-01-24T08:55:55.973835Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting deepface\n  Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting flask-cors>=4.0.1\n  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\nCollecting gunicorn>=20.1.0\n  Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.30.0 in /opt/conda/lib/python3.7/site-packages (from deepface) (4.64.1)\nRequirement already satisfied: tensorflow>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from deepface) (2.11.0)\nRequirement already satisfied: requests>=2.27.1 in /opt/conda/lib/python3.7/site-packages (from deepface) (2.28.2)\nRequirement already satisfied: keras>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from deepface) (2.11.0)\nCollecting opencv-python>=4.5.5.64\n  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting fire>=0.4.0\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: Flask>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from deepface) (2.2.3)\nCollecting retina-face>=0.0.1\n  Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\nRequirement already satisfied: Pillow>=5.2.0 in /opt/conda/lib/python3.7/site-packages (from deepface) (9.4.0)\nCollecting gdown>=3.10.1\n  Downloading gdown-4.7.3-py3-none-any.whl (16 kB)\nRequirement already satisfied: pandas>=0.23.4 in /opt/conda/lib/python3.7/site-packages (from deepface) (1.3.5)\nCollecting mtcnn>=0.1.0\n  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from deepface) (1.21.6)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire>=0.4.0->deepface) (2.2.0)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=1.1.2->deepface) (2.1.2)\nRequirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from Flask>=1.1.2->deepface) (2.2.3)\nRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=1.1.2->deepface) (4.11.4)\nRequirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=1.1.2->deepface) (8.1.3)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=1.1.2->deepface) (3.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown>=3.10.1->deepface) (1.16.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown>=3.10.1->deepface) (4.11.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown>=3.10.1->deepface) (3.9.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gunicorn>=20.1.0->deepface) (23.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.4->deepface) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.4->deepface) (2022.7.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.27.1->deepface) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.27.1->deepface) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.27.1->deepface) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.27.1->deepface) (3.4)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (3.8.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (15.0.6.1)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (4.4.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (1.51.1)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (23.1.21)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (59.8.0)\nRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (2.11.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (0.29.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (0.4.0)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.9.0->deepface) (2.11.2)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.38.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=1.1.2->deepface) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (3.4.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (1.35.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.3.2.post1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests>=2.27.1->deepface) (1.7.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (4.9)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (1.3.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=1.9.0->deepface) (3.2.2)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114261 sha256=7fa50c47d1491c97d773d60fc310e6f6b92eee778dfd0ef383f7c3a79de913fd\n  Stored in directory: /root/.cache/pip/wheels/3f/10/bd/e267fde82704057e3b60de87ee0ad877baee969d3bb50bced8\nSuccessfully built fire\nInstalling collected packages: protobuf, opencv-python, fire, mtcnn, gunicorn, gdown, flask-cors, retina-face, deepface\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.5.4.60\n    Uninstalling opencv-python-4.5.4.60:\n      Successfully uninstalled opencv-python-4.5.4.60\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 21.12.2 requires cupy-cuda115, which is not installed.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\ntfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\napache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.0 gdown-4.7.3 gunicorn-23.0.0 mtcnn-0.1.1 opencv-python-4.11.0.86 protobuf-3.19.6 retina-face-0.0.17\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (5.13.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly) (8.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (0.12.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from seaborn) (4.4.0)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.7/site-packages (from seaborn) (3.5.3)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.3.5)\nRequirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from seaborn) (1.21.6)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom PIL import Image\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom deepface import DeepFace\n\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nfrom plotly.subplots import make_subplots\nimport random\n\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom keras.models import *\nfrom keras.applications import *\nfrom keras.optimizers import *\nfrom keras import layers\nfrom keras.regularizers import *\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom keras import regularizers\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport plotly.graph_objs as go\nfrom sklearn.metrics import  roc_auc_score\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import LabelBinarizer\n\n\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, GlobalAveragePooling2D, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom deepface import DeepFace\nimport matplotlib.cm as cm\nplt.style.use('fivethirtyeight')\n","metadata":{"execution":{"iopub.status.busy":"2025-01-24T08:59:02.940961Z","iopub.execute_input":"2025-01-24T08:59:02.941645Z","iopub.status.idle":"2025-01-24T08:59:20.298110Z","shell.execute_reply.started":"2025-01-24T08:59:02.941601Z","shell.execute_reply":"2025-01-24T08:59:20.297237Z"},"trusted":true},"outputs":[{"name":"stdout","text":"25-01-24 08:59:15 - Directory /root/.deepface has been created\n25-01-24 08:59:15 - Directory /root/.deepface/weights has been created\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nclass Align_Face():\n    def __init__(self):\n        #definning opencv path for cascades\n        opencv_home = cv2.__file__\n        folders = opencv_home.split(os.path.sep)[0:-1]\n        path = folders[0]\n        for folder in folders[1:]:\n            path = path + \"/\" + folder\n\n        path_for_face = path+\"/data/haarcascade_frontalface_default.xml\"\n        path_for_eyes = path+\"/data/haarcascade_eye.xml\"\n        #loading Cascades\n        self.face_detection_cascade = cv2.CascadeClassifier(path_for_face)\n        self.eye_detection_cascade = cv2.CascadeClassifier(path_for_eyes)\n\n    def face_detection(self,img):\n        #getting bounding box of face\n        faces = self.face_detection_cascade.detectMultiScale(img, 1.5, 5)\n        if (len(faces) <= 0):\n            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            return img, img_gray\n        else:\n            X, Y, W, H = faces[0]\n            #getting Face ROI\n            img = img[int(Y):int(Y+H), int(X):int(X+W)]\n            return img, cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n\n\n    def trignometry_for_distance(self,a, b):\n        return math.sqrt(((b[0] - a[0]) * (b[0] - a[0])) +\\\n                ((b[1] - a[1]) * (b[1] - a[1])))\n\n    # Find eyes\n    def Face_Alignment(self,img_path):\n        img_raw = cv2.imread(img_path).copy()\n        img, gray_img = self.face_detection(cv2.imread(img_path))\n        #detecting Eyes\n        eyes = self.eye_detection_cascade.detectMultiScale(gray_img)\n\n        #checking if both eyes are present in the image\n        if len(eyes) >= 2:\n            eye = eyes[:, 2]\n            container1 = []\n            for i in range(0, len(eye)):\n                container = (eye[i], i)\n                container1.append(container)\n            df = pd.DataFrame(container1, columns=[\n                  \"length\", \"idx\"]).sort_values(by=['length'])\n            eyes = eyes[df.idx.values[0:2]]\n            \n\n            eye_1 = eyes[0]\n            eye_2 = eyes[1]\n            #selecting left and right eye on the basis of their position\n            if eye_1[0] > eye_2[0]:\n                left_eye = eye_2\n                right_eye = eye_1\n            else:\n                left_eye = eye_1\n                right_eye = eye_2\n\n\n            right_eye_center = (\n            int(right_eye[0] + (right_eye[2]/2)),\n            int(right_eye[1] + (right_eye[3]/2)))\n            right_eye_x = right_eye_center[0]\n            right_eye_y = right_eye_center[1]\n            cv2.circle(img, right_eye_center, 2, (255, 0, 0), 3)\n\n            left_eye_center = (int(left_eye[0] + (left_eye[2] / 2)),int(left_eye[1] + (left_eye[3] / 2)))\n            left_eye_x = left_eye_center[0]\n            left_eye_y = left_eye_center[1]\n            cv2.circle(img, left_eye_center, 2, (255, 0, 0), 3)\n\n\n            if left_eye_y > right_eye_y:\n#                 print(\"Rotate image to clock direction\")\n                point_3rd = (right_eye_x, left_eye_y)\n                direction = -1 \n            else:\n#                 print(\"Rotate to inverse clock direction\")\n                point_3rd = (left_eye_x, right_eye_y)\n                direction = 1 # rotate inverse direction of clock\n\n            cv2.circle(img, point_3rd, 2, (255, 0, 0), 2)\n            #calculating trignometric distance\n            a = self.trignometry_for_distance(left_eye_center,\n                        point_3rd)\n            b = self.trignometry_for_distance(right_eye_center,\n                        point_3rd)\n            c = self.trignometry_for_distance(right_eye_center,\n                        left_eye_center)\n            cos_a = (b*b + c*c - a*a)/(2*b*c)\n            angle = (np.arccos(cos_a) * 180) / math.pi\n            \n            #checking if we have to rotate image clock wise or anti clock wise\n            if direction == -1:\n                angle = 90 - angle\n            else:\n                angle = -(90-angle)\n\n            new_img = Image.fromarray(img_raw)\n            new_img = np.array(new_img.rotate(direction * angle))\n\n        return new_img","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:00:57.771639Z","iopub.execute_input":"2025-01-24T09:00:57.772316Z","iopub.status.idle":"2025-01-24T09:00:57.790844Z","shell.execute_reply.started":"2025-01-24T09:00:57.772280Z","shell.execute_reply":"2025-01-24T09:00:57.789600Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Load_Datasets():\n    '''This class takes these parameters as input\n    path :path is a required parameter to locate your dataset which have to load if their is train test path is alredy defined in your dataset directory feedit till there\n    else give path where the class folders are availaible\n    size_images  : with this parameter you can choose height and width of images\n    alignFace:if you want to align Face you can keep it True but remember it can ruin some images which are already aligned,we keep this paremeter off by default because our dataset images are already aligned.\n    minimum_images: if you want to give minimum images per class should have you can define here otherwise it will take minimum length of class by default and keep all clases with same length\n    thresh_minimum: if you want to restrict some any which have less images than this parameter so it will skip that class\n    '''\n    def __init__(self,path=\"/kaggle/input/combined-affectnet\",size_images= (96,96),gray=False,align_face = False,minimum_images = None,thresh_minimum = None):\n        self.path = path\n        self.gray = gray\n        self.size_images = size_images\n        self.align_face = align_face\n        self.al_face = Align_Face()\n        self.minimum_num_classes = minimum_images\n        self.thresh_minimum = thresh_minimum\n        self.flags = []\n    def load_images(self,p):\n        images = []\n        labels = []\n        #loading each emotion\n        for emotion in tqdm(os.listdir(p)):\n#             print(\"=\"*100)\n            print(emotion,\": Total Images Present in Dataset: \"+str(len(os.listdir(os.path.join(p,emotion)))) )\n#             print(\"=\"*100)\n            #checking if thresh minimum directed any emotion in black list(i mean any particluar class have lesss images than defined in thresh minimum it will be skip)\n            if emotion in self.flags:\n                print(f\"we are skipping this model {emotion} beacause total images present in this class: \"+str(len(os.listdir(os.path.join(p,emotion))))+f\" is less then thresh_minimum: {thresh_minimum}\" )\n                continue\n            #initialize parameter for legth of images of particular emotion\n            k = 0\n            new_p = os.path.join(p,emotion)\n            for image in os.listdir(new_p):\n                #checking if we are less than minimum_images_paramer to stop loading further images\n                if k<=self.minimum_num_classes:\n                \n                    try:\n                        #checking if align face argument is true or not\n                        if self.align_face:\n                            alignedFace = self.al_face.Face_Alignment(os.path.join(new_p,image))\n                            aligned_face, gray_aligned_face  = self.al_face.face_detection(alignedFace)\n                            if self.gray:\n                                image = gray_aligned_face\n                            else:\n                                image = aligned_face[:, :, ::-1]\n                        else:\n                            \n                            image = cv2.imread(os.path.join(new_p,image))\n                            image = image[:,:,::-1]\n#                             print(image.shape)\n                    except:\n#                         print(\"Error in this image \",emotion,image)\n#                         print(image)\n                        continue\n                    image = cv2.resize(image,self.size_images)\n                    images.append(image)\n                    labels.append(emotion)\n                    k+=1\n                else:\n                    #break loop if have reached the same length of minimum_image\n                    break\n        if self.gray:\n            images = np.array(images)\n            labels = np.array(labels)\n            images = images.reshape(images.shape[0],images.shape[1]*images.shape[2])\n            images = images.astype(\"float\")/255\n            \n        return images,labels\n    def load_data(self):\n        for file in os.listdir(self.path):\n            #if there is a train and test directory\n            if file.lower()==\"train\":\n                \n                print(\"*\"*100)\n                print(\"Loading Training Dataset\")\n                self.minimum_num_classes = None\n                train_path = os.path.join(self.path,\"train\")\n                #checking the class which have less number of images\n                for i in os.listdir(train_path):\n                    if  self.minimum_num_classes:\n                        self.minimum_num_classes = min(len(os.listdir(os.path.join(train_path,i))),self.minimum_num_classes)\n                    else:\n                         self.minimum_num_classes = len(os.listdir(os.path.join(train_path,i) ) )\n                    if self.thresh_minimum != None:\n                        if len(os.listdir(os.path.join(train_path,i)))<self.thresh_minimum:\n                            self.flags.append(i)\n                train_x,train_y = self.load_images(train_path)\n            else:\n                if file.lower()==\"test\":\n                    print(\"*\"*100)\n                    print(\"Loading Testing Dataset\")\n                    self.minimum_num_classes = None\n                    test_path = os.path.join(self.path,\"test\")\n                     #checking the class which have less number of images\n                    for i in os.listdir(test_path):\n                        if  self.minimum_num_classes:\n                            self.minimum_num_classes = min(len(os.listdir(os.path.join(test_path,i))),self.minimum_num_classes)\n                        else:\n                             self.minimum_num_classes = len(os.listdir(os.path.join(test_path,i) ) )\n                        if self.thresh_minimum != None:\n                            if len(os.listdir(os.path.join(test_path,i)))<self.thresh_minimum:\n                                self.flags.append(i)\n                       \n                    test_x,test_y = self.load_images(test_path)\n                #if there is no train test directory so directly load dataset with emotions\n                else:\n                    print(\"*\"*100)\n                    print(\"Loading Dataset Directly | No Train-Test Folders\")\n                    #checking the class which have less number of images\n                    for i in os.listdir(self.path):\n                        if  self.minimum_num_classes:\n                            self.minimum_num_classes = min(len(os.listdir(os.path.join(self.path,i))),self.minimum_num_classes)\n                        else:\n                             self.minimum_num_classes = len(os.listdir(os.path.join(self.path,i) ) )\n                        if self.thresh_minimum != None:\n                            if len(os.listdir(os.path.join(self.path,i)))<self.thresh_minimum:\n                                self.flags.append(i)\n                    x,y = self.load_images(self.path)\n                    return x,y,None,None\n        return train_x,train_y,test_x,test_y","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:02:05.642799Z","iopub.execute_input":"2025-01-24T09:02:05.643135Z","iopub.status.idle":"2025-01-24T09:02:05.663179Z","shell.execute_reply.started":"2025-01-24T09:02:05.643108Z","shell.execute_reply":"2025-01-24T09:02:05.662350Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data = Load_Datasets(size_images = (224, 224),minimum_images=50)\nx,y,_,_= data.load_data()\n\n# data = Load_Datasets(minimum_images=1000,size_images = (96, 96),align_face=False)\n# openfacex,openfacey,test_x,test_y = data.load_data()\n\n\n# data = Load_Datasets(minimum_images=1000,size_images = (55, 47),align_face=False)\n# deepidx,deepidy,test_x,test_y = data.load_data()\n\n\n# data = Load_Datasets(minimum_images=1000,size_images = (152, 152,),align_face=False)\n# deepfacex,deepfacey,test_x,test_y = data.load_data()\n\n\n# data = Load_Datasets(minimum_images=1000,size_images = (160, 160),align_face=False)\n# facenetx,facenety,test_x,test_y = data.load_data()\n","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:02:13.674127Z","iopub.execute_input":"2025-01-24T09:02:13.674997Z","iopub.status.idle":"2025-01-24T09:02:13.720289Z","shell.execute_reply.started":"2025-01-24T09:02:13.674957Z","shell.execute_reply":"2025-01-24T09:02:13.719263Z"},"trusted":true},"outputs":[{"name":"stdout","text":"****************************************************************************************************\nLoading Dataset Directly | No Train-Test Folders\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 137.95it/s]","output_type":"stream"},{"name":"stdout","text":"affectnet-cleaned : Total Images Present in Dataset: 2\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df= pd.DataFrame()\n\ndf[\"labels\"] =y","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:02:19.348159Z","iopub.execute_input":"2025-01-24T09:02:19.349094Z","iopub.status.idle":"2025-01-24T09:02:19.371339Z","shell.execute_reply.started":"2025-01-24T09:02:19.349057Z","shell.execute_reply":"2025-01-24T09:02:19.370483Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def plot_data_distribution(data,label):\n    fig = make_subplots(\n        rows=1, cols=2,\n        column_widths=[0.5, 0.5],\n        row_heights=[0.5],\n        specs=[[ {\"type\": \"pie\"}, {\"type\": \"Funnelarea\"}]])\n    #plotting Pie Plot\n\n    fig.add_trace(go.Pie(\n        labels=data.index, \n        values=data[\"labels\"],\n        legendgroup=\"group\",\n        textinfo='percent+label'), \n        row=1, col=1)\n    #Plotting Funnel Area\n    fig.add_trace(go.Funnelarea(\n       values=data['labels'], labels=data.index, name='Emotions data distribution',\n        title = {\"position\": \"top center\",}), \n                        row=1, col=2)\n\n    fig.update_layout(height=500,width=1000, bargap=0.2,\n                      margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                      title_text=f\"{label} Data Distribution\",\n                      template=\"plotly_white\",\n                      title_font=dict(size=29, color='#8a8d93', family=\"Lato, sans-serif\"),\n                      font=dict(color='#8a8d93'), \n                      hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                      showlegend=False)\n    fig.show()\ndata_dist = pd.DataFrame(df[\"labels\"].value_counts() )\nplot_data_distribution(data_dist,\"AffectNet\")","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:02:21.637246Z","iopub.execute_input":"2025-01-24T09:02:21.637965Z","iopub.status.idle":"2025-01-24T09:02:23.167796Z","shell.execute_reply.started":"2025-01-24T09:02:21.637928Z","shell.execute_reply":"2025-01-24T09:02:23.166864Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"2d39d5fe-210e-417c-8fc9-44c4df05a41c\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2d39d5fe-210e-417c-8fc9-44c4df05a41c\")) {                    Plotly.newPlot(                        \"2d39d5fe-210e-417c-8fc9-44c4df05a41c\",                        [{\"labels\":[],\"legendgroup\":\"group\",\"textinfo\":\"percent+label\",\"values\":[],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]}},{\"labels\":[],\"name\":\"Emotions data distribution\",\"title\":{\"position\":\"top center\"},\"values\":[],\"type\":\"funnelarea\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,1.0]}}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"margin\":{\"b\":0,\"r\":20,\"l\":20},\"xaxis\":{\"tickmode\":\"linear\"},\"title\":{\"font\":{\"size\":29,\"color\":\"#8a8d93\",\"family\":\"Lato, sans-serif\"},\"text\":\"AffectNet Data Distribution\"},\"font\":{\"color\":\"#8a8d93\"},\"hoverlabel\":{\"font\":{\"size\":13,\"family\":\"Lato, sans-serif\"},\"bgcolor\":\"#f2f2f2\"},\"height\":500,\"width\":1000,\"bargap\":0.2,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('2d39d5fe-210e-417c-8fc9-44c4df05a41c');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#This function is build forgetting random sample each time for each class\ndef get_random_unique_sample(images,labels):\n    l = []\n    imgs = []\n    labels_array = np.array(labels)\n    #unique labels\n    uniq = list(np.unique(labels_array))\n    #determining images and adding into list until we get the our unique images for each emotion \n    while len(l)!=len(uniq):\n        r = random.randint(0,len(images)-1)\n        unknown = labels[r]\n        if unknown not in l:\n            l.append(labels[r])\n            imgs.append(images[r])\n    del labels_array\n    del uniq\n    return  imgs,l\ndef show_images(images, rows = 1, titles = None,main_title = None):\n    assert((titles is None) or (len(images) == len(titles)))\n    #for potting Images\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    if main_title:\n        fig.suptitle(main_title,fontsize = 35)\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(int(rows),int( np.ceil(n_images/float(rows))), n +1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n\n    plt.subplots_adjust(wspace=0, hspace=0)\n    fig.tight_layout()\n    plt.show()\nimgss,labelss = get_random_unique_sample(x,y)\nimgss,labelss = np.array(imgss),np.array(labelss)\nshow_images(imgss, rows = 2, titles = labelss,main_title = \"AffectNet Dataset\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:02:27.680424Z","iopub.execute_input":"2025-01-24T09:02:27.681368Z","iopub.status.idle":"2025-01-24T09:02:27.696013Z","shell.execute_reply.started":"2025-01-24T09:02:27.681333Z","shell.execute_reply":"2025-01-24T09:02:27.695172Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 0x0 with 0 Axes>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def preprocess(x,y,name):\n    print(\"*\"*20,name,\"*\"*20)\n    x,y = np.array(x),np.array(y)\n    unique_labels = list(np.unique(y))\n    encoded = []\n    # craeting encoded labels into numeric number 0:angry etc \n    etol = {k:v for k,v in enumerate(unique_labels)}\n    #encoding each label \n    for label in y:\n        encoded.append(unique_labels.index(label))\n#     encoder = LabelEncoder()\n#     y = encoder.fit_transform(y)\n    #one hot encoding of each label from 0 -->0000 0000, 1-->0000 0001,2--> 0000 0010 etc\n    y = to_categorical(np.array(encoded))\n    #splitting training and testing dataset \n    train_x,test_x,train_y,test_y  =train_test_split(x,y,test_size=0.2,shuffle=True)\n    print(len(train_x),len(train_y),len(test_x),len(test_y))\n    print(\"ShapeS Train:\",train_x.shape,train_y.shape)\n    print(\"ShapeS Test:\",test_x.shape,test_y.shape)\n\n    return train_x,test_x,train_y,test_y,etol\ntrain_x,test_x,train_y,test_y,etol =preprocess(x,y,\"Data\")\n# open_train_x,open_test_x,open_train_y,open_test_y =preprocess(openfacex,openfacey,\"OpenFace\")\n# deepface_train_x,deepface_test_x,deepface_train_y,deepface_test_y =preprocess(deepfacex,deepfacey,\"DeepFace\")\n# facenet_train_x,facenet_test_x,facenet_train_y,facenet_test_y =preprocess(facenetx,facenety,\"Facenet\")\n# deepid_train_x,deepid_test_x,deepid_train_y,deepid_test_y =preprocess(deepidx,deepidy,\"DeepID\")","metadata":{"execution":{"iopub.status.busy":"2025-01-24T09:02:31.904621Z","iopub.execute_input":"2025-01-24T09:02:31.905276Z","iopub.status.idle":"2025-01-24T09:02:32.008237Z","shell.execute_reply.started":"2025-01-24T09:02:31.905243Z","shell.execute_reply":"2025-01-24T09:02:32.007064Z"},"trusted":true},"outputs":[{"name":"stdout","text":"******************** Data ********************\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2621225113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0metol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0metol\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# open_train_x,open_test_x,open_train_y,open_test_y =preprocess(openfacex,openfacey,\"OpenFace\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# deepface_train_x,deepface_test_x,deepface_train_y,deepface_test_y =preprocess(deepfacex,deepfacey,\"DeepFace\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/2621225113.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     y = encoder.fit_transform(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#one hot encoding of each label from 0 -->0000 0000, 1-->0000 0001,2--> 0000 0010 etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#splitting training and testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2753\u001b[0m     \"\"\"\n\u001b[1;32m   2754\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2755\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"],"ename":"ValueError","evalue":"zero-size array to reduction operation maximum which has no identity","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"class Model_Building:\n    def __init__(self,model_name=None,train_x=None,train_y=None,test_x=None,test_y=None,model_check_point = \"/kaggle/working/\"):\n        # Constructor for the Model_Building class\n        # Initializes the class attributes\n        print(\"-\"*20,\"Model Name: \" ,model_name.upper(),\"-\"*20)\n        self.model_name = model_name\n        self.check_point = model_check_point\n        # Sets the base model name\n        if self.model_name == \"vgg19\":\n            self.preprocess_input = keras.applications.vgg19.preprocess_input\n            self.model = VGG19\n        elif self.model_name == \"xception\":\n            self.preprocess_input = keras.applications.xception.preprocess_input\n            self.model = Xception\n        elif self.model_name==\"mobilenetv2\":\n            self.preprocess_input = keras.applications.mobilenet_v2.preprocess_input\n            self.model = MobileNetV2\n        elif self.model_name == \"efficient\":\n            self.preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n            self.model = EfficientNetB7\n        else:\n            self.preprocess_input = keras.applications.inception_v3.preprocess_input\n            self.model = InceptionV3\n        # Sets the preprocessing function and the base model based on the model name provided\n        self.train_x = train_x\n        self.train_y = train_y\n        self.test_x = test_x\n        self.test_y = test_y\n        # Sets the training and testing data\n        self.main_model = None\n        # Initializes the main model to None\n    \n    def base_model(self,data,pr_model  ):\n        # Builds the base model by extracting the features from the pre-trained model\n        w,h = data.shape[1:3]\n        # Extracts the width and height from the shape of the data\n        cnn_model = pr_model(include_top=False, input_shape=(w, h, 3), weights='imagenet')\n        # Initializes the pre-trained model without the fully connected layers\n        \n        inputs = Input((w, h, 3))\n        x = inputs\n        x = Lambda(self.preprocess_input, name='preprocessing')(x)\n        # Applies the preprocessing function to the input data\n        x = cnn_model(x)\n        # Extracts the features from the pre-trained model\n        x = GlobalAveragePooling2D()(x)\n        # Performs global average pooling to reduce the number of parameters\n        cnn_model = Model(inputs, x)\n        # Sets the base model as the input and the extracted features\n        features = cnn_model.predict(data, batch_size=5, verbose=1)\n        # Extracts the features from the training or testing data\n        return features\n    \n        # Define a function for the bottom model of the neural network\n    def bottom_model(self,features):\n        # Determine the percentage of data to be used for training and rest for validation\n\n        # Define the neural network architecture\n        model = Sequential()\n        model.add(layers.Dense(300, activation=\"relu\", input_shape=(features.shape[1],)))\n        model.add(layers.Dense(200, activation=\"relu\"))\n        model.add(BatchNormalization())\n#         model.add(layers.Dense(1500, activation=\"relu\"))\n        model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n        model.add(Dropout(0.2))\n        model.add(layers.Dense(len(np.unique(train_y, axis=0)), activation=\"softmax\"))\n\n        # Compile the model with appropriate settings\n        model.compile(optimizer=Adam(0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n        # Train the model on the specified data\n        \n\n        # Return the trained model and its history\n        return model\n\n\n    # Define a function for running the entire neural network model\n    def forward(self, batch_size=4, epochs=10, train_percent=0.90, verbose=1):\n        # Extract features using the base model\n        f = self.base_model(self.train_x,self.model)\n        percent = int(len(f) * train_percent)\n\n        path1 = os.path.join(self.check_point,self.model_name)\n        # Train the bottom model using the extracted features\n        if not os.path.exists(path1):\n            os.mkdir(path1)\n        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(path1,\"cp-{epoch:04d}.ckpt\"), \n        verbose=verbose, \n        save_weights_only=True,\n        save_freq=5*batch_size)\n        self.main_model = self.bottom_model(f)\n        self.main_model.save_weights(self.check_point.format(epoch=0))\n        hist = self.main_model.fit(f[:percent], train_y[:percent], validation_data=(f[percent:], train_y[percent:]),\n                batch_size=batch_size, epochs=epochs, callbacks=[cp_callback])\n#         model.save(f\"/kaggle/working/{self.model_name}.h5\")\n        \n        # Return the history of the trained model and the trained model itself\n        return hist, self.main_model\n\n\n    # Define a function for making predictions with the trained neural network model\n    def predict(self, sample_test=None):\n        if sample_test is None:\n            # Extract features using the base model for the test data\n            test_f = self.base_model(self.test_x,self.model)\n\n            # Make predictions using the trained model\n            p = self.main_model.predict(test_f)\n            p = np.argmax(p, axis=1)\n        else:\n            # Extract features using the base model for the given input data\n            test_f = self.base_model( sample_test,self.model)\n\n            # Make predictions using the trained model\n            p = self.main_model.predict(test_f)\n            p = np.argmax(p, axis=1)\n\n        # Return the predicted classes\n        return p\n\n\n    # Define a function for evaluating the performance of the trained neural network model\n    def evaluate(self):\n        # Extract features using the base model for the test data\n        test_f = self.base_model(self.test_x,self.model)\n\n        # Evaluate the performance of the trained model on the test data\n        loss, acc = self.main_model.evaluate(test_f, self.test_y)\n\n        # Print the results\n        print(\"=\"*50, self.model_name, \"=\"*50)\n        print(\"On Test Data\")\n        print(\"Loss: \", round(loss, 4))\n        print(\"Accuracy: \", round(acc, 4))\n\n        # Return the loss and accuracy of the trained model on the test data\n        return loss, acc\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T08:52:41.43562Z","iopub.execute_input":"2023-03-17T08:52:41.436253Z","iopub.status.idle":"2023-03-17T08:52:41.470708Z","shell.execute_reply.started":"2023-03-17T08:52:41.436207Z","shell.execute_reply":"2023-03-17T08:52:41.469055Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Experiment Trying to check if testing accuracy increase with concatnation of two Models(Function API)\n# def get_features(pr_model, data,width=224):\n#     #base model\n#     cnn_model = pr_model(include_top=False, input_shape=(width, width, 3), weights='imagenet')\n    \n#     inputs = Input((width, width, 3))\n#     x = inputs\n#     x = Lambda(preprocess_input, name='preprocessing')(x)\n#     x = cnn_model(x)\n#     x = GlobalAveragePooling2D()(x)\n#     cnn_model = Model(inputs, x)\n#     features = cnn_model.predict(data, batch_size=5, verbose=1)\n#     return features\n# inception_features = get_features(InceptionV3, train_x)\n# xception_features = get_features(Xception,train_x)\n# #concating two pretrained models features extracted from training data \n# features = np.concatenate([inception_features, xception_features],axis=1)\n\n# model = Sequential()\n# model.add(layers.Dense(200,activation= \"relu\",input_shape= (features.shape[1],)))\n# # model.add(layers.Dense(900,activation = \"relu\"))\n# model.add(layers.Dense(100,activation=\"relu\"))\n# model.add(Dropout(0.5))\n# #model.add(layers.Dense(700,activation=\"relu\"))\n# model.add(layers.Dense(8,activation=\"softmax\"))\n# model.compile(optimizer = Adam(0.0001) ,loss = \"categorical_crossentropy\" , metrics = [\"accuracy\"])\n# train_percent = 0.90\n# percent = int(len(features)*train_percent)\n# history = model.fit(features[:percent],train_y[:percent],validation_data = (features[percent:],train_y[percent:]),batch_size = 5,epochs = 10)\n\n# inception_features = get_features(InceptionV3, test_x)\n# xception_features = get_features(Xception,test_x)\n# features = np.concatenate([inception_features, xception_features],axis=1)\n# eva = model.evaluate(features,test_y)\n# print(eva[0],eva[1])","metadata":{"execution":{"iopub.status.busy":"2023-03-17T08:52:41.473404Z","iopub.execute_input":"2023-03-17T08:52:41.474168Z","iopub.status.idle":"2023-03-17T08:52:41.491434Z","shell.execute_reply.started":"2023-03-17T08:52:41.474115Z","shell.execute_reply":"2023-03-17T08:52:41.489488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_Accuracy_Loss(history,name):\n    plt.style.use(\"fivethirtyeight\")\n    plt.figure(figsize = (20,6))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'])\n    plt.title(f\" {name} Model Loss\",fontsize=20)\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.legend(['train loss', 'validation loss'], loc ='best')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'])\n    plt.title(f\"{name} Model Accuracy \",fontsize=20)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend(['training accuracy', 'validation accuracy'], loc ='best')\n    plt.show()\n\ndef Plot_ROC(y_pred,y_test,etol = {0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sad', 7: 'surprise'}):\n    \n    \"\"\"\n    This function takes in predicted and original labels, performs multiclass ROC analysis and\n    plots a ROC curve using Plotly.\n    \n    Parameters:\n    y_pred (array-like): Predicted labels\n    y_test (array-like): True labels\n    \n    Returns:\n    None\n    \"\"\"\n    \n    # Convert labels to binary format\n    lb = LabelBinarizer()\n    lb.fit(y_test)\n    y_test = lb.transform(y_test)\n    y_pred = lb.transform(y_pred)\n    \n    # Compute ROC curve and ROC area for each class\n    n_classes = y_test.shape[1]\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        \n    # Plot ROC curve for each class using Plotly\n    fig = go.Figure()\n    for i in range(n_classes):\n        fig.add_trace(\n            go.Scatter(x=fpr[i], y=tpr[i],\n                       name=f'ROC curve (class {list(etol.values())[i]})',\n                       line=dict(width=2))\n        )\n        \n    # Add diagonal line for reference\n    fig.add_trace(\n        go.Scatter(x=[0, 1], y=[0, 1],\n                   name='Diagonal line',\n                   line=dict(color='gray', dash='dash'))\n    )\n    \n    # Set plot title and axes labels\n    fig.update_layout(\n        title='Multiclass ROC Curve',\n        xaxis=dict(title='False Positive Rate'),\n        yaxis=dict(title='True Positive Rate')\n    )\n    \n    # Show plot\n    fig.show()\n\ndef plot_ROC(y_scores,y_onehot,etol = {0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sad', 7: 'surprise'}):\n    fig = go.Figure()\n    fig.add_shape(\n        type='line', line=dict(dash='dash'),\n        x0=0, x1=1, y0=0, y1=1\n    )\n\n    for i in range(y_scores.shape[1]):\n        y_true = y_onehot[:, i]\n        y_score = y_scores[:, i]\n\n        fpr, tpr, _ = roc_curve(y_true, y_score)\n        auc_score = roc_auc_score(y_true, y_score)\n\n        name = f\"{list(etol.values())[i]} (AUC={auc_score:.2f})\"\n        fig.add_trace(go.Scatter(x=fpr, y=tpr,name=name, mode='lines'))\n\n    fig.update_layout(\n        xaxis_title='False Positive Rate',\n        yaxis_title='True Positive Rate',\n        yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n        xaxis=dict(constrain='domain'),\n        width=700, height=500\n    )\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:15:05.211296Z","iopub.execute_input":"2023-03-17T09:15:05.211979Z","iopub.status.idle":"2023-03-17T09:15:05.237307Z","shell.execute_reply.started":"2023-03-17T09:15:05.211918Z","shell.execute_reply":"2023-03-17T09:15:05.236225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#defineing object of each pretrained model\nincept = Model_Building(\"inception\",train_x,train_y,test_x,test_y )\nincept_hist,incept_model = incept.forward(verbose = 1,batch_size = 16,epochs =70)\nincept_loss,incept_acc = incept.evaluate()\n\nxcept = Model_Building(\"xception\",train_x,train_y,test_x,test_y )\nxcept_hist,xcept_model = xcept.forward(verbose = 1,batch_size = 16,epochs =70)\nxcept_loss,xcept_acc = xcept.evaluate()\n\nmobilenet = Model_Building(\"mobilenetv2\",train_x,train_y,test_x,test_y )\nmobilenet_hist,mobilenet_model = mobilenet.forward(verbose = 1,batch_size = 16,epochs =70)\nmobilenet_loss,mobilenet_acc = mobilenet.evaluate()\n\nvgg = Model_Building(\"vgg19\",train_x,train_y,test_x,test_y )\nvgg_hist,vgg_model = vgg.forward(verbose = 1,batch_size = 16,epochs =70)\nvgg_loss,vgg_acc = vgg.evaluate()\n\nefficient = Model_Building(\"efficient\",train_x,train_y,test_x,test_y )\nefficient_hist,efficient_model = efficient.forward(verbose = 1,batch_size = 16,epochs =70)\nefficient_loss,efficient_acc = efficient.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:15:56.193712Z","iopub.execute_input":"2023-03-17T09:15:56.194174Z","iopub.status.idle":"2023-03-17T09:16:36.515233Z","shell.execute_reply.started":"2023-03-17T09:15:56.19412Z","shell.execute_reply":"2023-03-17T09:16:36.513798Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_Accuracy_Loss(incept_hist,\"Inception\")\nplot_Accuracy_Loss(xcept_hist,\"Xception\")\nplot_Accuracy_Loss(mobilenet_hist,\"MobileNet\")\nplot_Accuracy_Loss(vgg_hist,\"VGG\")\nplot_Accuracy_Loss(efficient_hist,\"Efficient\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:16:36.517686Z","iopub.execute_input":"2023-03-17T09:16:36.518135Z","iopub.status.idle":"2023-03-17T09:16:37.000195Z","shell.execute_reply.started":"2023-03-17T09:16:36.518094Z","shell.execute_reply":"2023-03-17T09:16:36.998805Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(model_obj,d={0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sad', 7: 'surprise'},test_x=None,size = (224,224),model_path = None,save_img = None):\n    '''if you are predicting from saved model , \n    saved model should have their name directory in which models chekpoints are present'''\n    print(\"Predicted Emotions\")\n    #if there is no text_x(test images array) given so it will take it defined model attribute\n    if model_path is None:\n        #predict the test images from define model\n        results = model_obj.predict()\n        l = []\n        #inversing the encoded predicted labels form dictinary we defined above\n        for i in results:\n            l.append(d[i])\n        #just showing the sample of each class of predicted output\n        imgs,la = get_random_unique_sample(model_obj.test_x,l)\n        show_images(imgs,titles=la,rows=2,main_title = \"Predicted Emotions by \"+model_obj.model_name)\n        \n        orginal_labels = []\n        #inversing the one hot encoded labels of orginal labels\n        or_label = np.argmax(model_obj.test_y,axis=1)\n        #inversing numeric original labels into actual name \n        for i in or_label:\n            orginal_labels.append(d[i])\n        #returning test_x,predicted labes and original labels\n        return model_obj.test_x,l, orginal_labels\n    else:\n        #if only one image path is given in parameter\n        print(\"Predicting from Images Paths\")\n        if type(test_x)==str:\n            img = cv2.imread(test_x)\n            imgs = cv2.resize(img,size)\n            imgs = np.expand_dims(imgs,axis= 0)\n            print(imgs.shape)\n            \n        else:\n            #array of images is given in parameter\n            imgs = []\n            for img in test_x:\n                img = cv2.imread(img)\n                imgs.append(cv2.resize(img,size))\n            imgs = np.array(imgs)\n        model_obj = Model_Building(model_name = model_obj)\n        path1 = os.path.join(model_path,model_obj.model_name,\"cp-{epoch:04d}.ckpt\")\n        latest = tf.train.latest_checkpoint(os.path.dirname(path1))\n        test_f = model_obj.base_model( imgs,model_obj.model)\n        model  = model_obj.bottom_model(test_f)\n        # Load the previously saved weights\n        print(latest)\n        model.load_weights(latest)\n        \n        results = model.predict(test_f)\n        results = np.argmax(results, axis=1)\n        final_img_emotions = []\n        l = []\n        for i in results:\n            l.append(d[i])\n        for pos,img in enumerate(imgs) :\n            img = cv2.putText(img, f'Predicted Emotion: {l[pos]}', (5,15),cv2.FONT_HERSHEY_SIMPLEX, \n                   0.5, (0,0,0), 1, cv2.LINE_AA)\n            if save_img:\n                cv2.imwrite(save_img+str(results[pos])+f\"_{pos}.jpg\",img[:,:,::-1])\n            final_img_emotions.append(img)\n       \n        if len(l)==1:\n            plt.imshow(final_img_emotions[0])\n            plt.title(l[0])\n            \n        else:\n            \n            imgss,la = get_random_unique_sample(final_img_emotions,l)\n            \n            #just showing the sample of each class of predicted output\n\n            show_images(imgss,titles=la,rows=2,main_title = \"Predicted Emotions \")\n        \n#         orginal_labels = []\n#         or_label = np.argmax(test_y,axis=-1)\n#         for i in or_label:\n#             orginal_labels.append(d[i])\n        return final_img_emotions, l \n#predicting from images paths loading save models\npred_image,pred_l = predict(model_obj = \"inception\",d = etol,test_x = [\"/kaggle/input/young-affectnet-hq/surprise/10_ffhq_101.png\",\"/kaggle/input/young-affectnet-hq/neutral/10_ffhq_1.png\"],model_path = \"/kaggle/working/\",save_img= \"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:20:02.398243Z","iopub.execute_input":"2023-03-17T09:20:02.398704Z","iopub.status.idle":"2023-03-17T09:20:08.973605Z","shell.execute_reply.started":"2023-03-17T09:20:02.398666Z","shell.execute_reply":"2023-03-17T09:20:08.971635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg_testx,vgg_pred,vgg_testy = predict(vgg,etol)\nPlot_ROC(vgg_pred,vgg_testy)\nprint ('\\n*\\t\\tClassification Report VGG:\\n', classification_report(vgg_testy,vgg_pred) )","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:22:36.654436Z","iopub.execute_input":"2023-03-17T09:22:36.654957Z","iopub.status.idle":"2023-03-17T09:22:36.664023Z","shell.execute_reply.started":"2023-03-17T09:22:36.654914Z","shell.execute_reply":"2023-03-17T09:22:36.661705Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xcept_testx,xcept_pred,xcept_testy = predict(xcept,etol)\nPlot_ROC(xcept_pred,xcept_testy)\nprint ('\\n*\\t\\tClassification Report Xception:\\n', classification_report(xcept_testy,xcept_pred) )","metadata":{"execution":{"iopub.status.busy":"2023-03-16T16:23:28.557218Z","iopub.execute_input":"2023-03-16T16:23:28.558001Z","iopub.status.idle":"2023-03-16T16:23:28.594931Z","shell.execute_reply.started":"2023-03-16T16:23:28.557956Z","shell.execute_reply":"2023-03-16T16:23:28.593346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mobilenet_testx,mobilenet_pred,mobilenet_testy= predict(mobilenet,etol)\nPlot_ROC(mobilenet_pred,mobilenet_testy)\nprint ('\\n*\\t\\tClassification Report MobileNet:\\n', classification_report(mobilenet_testy,mobilenet_pred) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efficient_testx,efficient_pred,efficient_testy = predict(efficient,etol)\nPlot_ROC(efficient_pred,efficient_testy)\nprint ('\\n*\\t\\tClassification Report Efficient:\\n', classification_report(efficient_testy,efficient_pred) )","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:31:49.868315Z","iopub.execute_input":"2023-03-16T14:31:49.868647Z","iopub.status.idle":"2023-03-16T14:34:10.39301Z","shell.execute_reply.started":"2023-03-16T14:31:49.868619Z","shell.execute_reply":"2023-03-16T14:34:10.392041Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"incept_testx,incept_pred,incept_testy = predict(incept,etol)\nPlot_ROC(incept_pred,incept_testy)\nprint ('\\n*\\t\\tClassification Report Incption:\\n', classification_report(incept_testy,incept_pred) )","metadata":{"execution":{"iopub.status.busy":"2023-03-17T09:23:58.773779Z","iopub.execute_input":"2023-03-17T09:23:58.774194Z","iopub.status.idle":"2023-03-17T09:24:12.249192Z","shell.execute_reply.started":"2023-03-17T09:23:58.774159Z","shell.execute_reply":"2023-03-17T09:24:12.247783Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.figure(figsize = (20,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(names))\n    plt.xticks(tick_marks, names, rotation=90)\n    plt.yticks(tick_marks, names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\ndef plot_CM(pred_y,original_y,name = None):\n    if name is not None:\n        print(\"*\"*30,name.upper(),\"*\"*30)\n    cm = confusion_matrix(original_y, pred_y)\n    np.set_printoptions(precision=2)\n#     print(cm)\n    plt.rcParams[\"figure.figsize\"]=(8,5)\n    plt.figure()\n    if name:\n        plot_confusion_matrix(cm,list(etol.values()),title = name)\n    else:\n         plot_confusion_matrix(cm,list(etol.values()))\n\ndef plot_CM( pred_y,original_y,title=\"Confusion Matrix\",categories = etol):\n    plt.figure(figsize = (20,10))\n    p = random.randint(0,1)\n    k = [plt.cm.Greens,plt.cm.Blues]\n    ax= plt.subplot()\n    cf_matrix = confusion_matrix(original_y, pred_y)\n    sns.heatmap(cf_matrix, annot=True,ax=ax,cmap = k[p])\n    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n    ax.set_title(title); \n    ax.xaxis.set_ticklabels(list(etol.values())); ax.yaxis.set_ticklabels(list(etol.values()));\nplot_CM(efficient_pred,efficient_testy,\"Efficient\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:34:10.394998Z","iopub.execute_input":"2023-03-16T14:34:10.39538Z","iopub.status.idle":"2023-03-16T14:34:11.027567Z","shell.execute_reply.started":"2023-03-16T14:34:10.395351Z","shell.execute_reply":"2023-03-16T14:34:11.026562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_CM(mobilenet_pred,mobilenet_testy,\"Mobilenet\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_CM(incept_pred,incept_testy,\"Inception\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_CM(xcept_pred,xcept_testy,\"Xception\")\n","metadata":{"execution":{"iopub.status.busy":"2023-03-16T12:28:39.0656Z","iopub.execute_input":"2023-03-16T12:28:39.066479Z","iopub.status.idle":"2023-03-16T12:28:39.696916Z","shell.execute_reply.started":"2023-03-16T12:28:39.066445Z","shell.execute_reply":"2023-03-16T12:28:39.69598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_CM(vgg_pred,vgg_testy,\"VGG19\")","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:34:11.028788Z","iopub.execute_input":"2023-03-16T14:34:11.02908Z","iopub.status.idle":"2023-03-16T14:34:11.643844Z","shell.execute_reply.started":"2023-03-16T14:34:11.029054Z","shell.execute_reply":"2023-03-16T14:34:11.643017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass GradCAM:\n    def __init__(self, model_name=\"xception\", img_size=(299, 299)):\n        self.img_size = img_size\n        self.preprocess_input = None\n        self.decode_predictions = None\n        self.last_conv_layer_name = None\n        self.model_builder = None\n        self.model = None\n        \n        if model_name == \"inception\":\n            self.preprocess_input = keras.applications.inception_v3.preprocess_input\n            self.decode_predictions = keras.applications.inception_v3.decode_predictions\n            self.last_conv_layer_name = \"mixed10\"\n            self.model_builder = keras.applications.inception_v3.InceptionV3\n        elif model_name == \"mobilenetv2\":#224,224\n            self.preprocess_input = keras.applications.mobilenet_v2.preprocess_input\n            self.decode_predictions = keras.applications.mobilenet_v2.decode_predictions\n            self.last_conv_layer_name = \"Conv_1\"\n            self.model_builder = keras.applications.mobilenet_v2.MobileNetV2\n        elif model_name == \"efficient\":#600,600\n            self.preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n            self.decode_predictions = tf.keras.applications.efficientnet.decode_predictions\n            self.last_conv_layer_name = \"top_conv\"\n            self.model_builder = tf.keras.applications.EfficientNetB7\n        elif model_name == \"vgg19\":#299,299\n            self.preprocess_input = keras.applications.vgg19.preprocess_input\n            self.decode_predictions = keras.applications.vgg19.decode_predictions\n            self.last_conv_layer_name = \"block5_conv4\"\n            self.model_builder = keras.applications.vgg19.VGG19\n        else:#299,299\n            self.preprocess_input = keras.applications.xception.preprocess_input\n            self.decode_predictions = keras.applications.xception.decode_predictions\n            self.last_conv_layer_name = \"block14_sepconv2_act\"\n            self.model_builder = keras.applications.xception.Xception\n\n    def get_img_array(self, img):\n        array = cv2.resize(img, self.img_size)\n        array = np.expand_dims(array, axis=0)\n        return array\n\n    def make_gradcam_heatmap(self, img_array, pred_index=None):\n        #loading model for gradient\n        grad_model = tf.keras.models.Model(\n            [self.model.inputs], [self.model.get_layer(self.last_conv_layer_name).output, self.model.output]\n        )\n\n      #taking gradient tape from model weights\n        with tf.GradientTape() as tape:\n            last_conv_layer_output, preds = grad_model(img_array)\n            if pred_index is None:\n                pred_index = tf.argmax(preds[0])\n            class_channel = preds[:, pred_index]\n\n        #extracted Gradients\n        grads = tape.gradient(class_channel, last_conv_layer_output)\n\n        \n        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n        #last pooled layer\n\n        last_conv_layer_output = last_conv_layer_output[0]\n        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n        heatmap = tf.squeeze(heatmap)\n\n        #creating heatmap of the gradients\n        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n        return heatmap.numpy()\n\n\n    def display_gradcam(self,img, heatmap,  alpha=0.4):\n\n\n\n        heatmap = np.uint8(255 * heatmap)     \n        jet = cm.get_cmap(\"jet\")\n\n        #coloring the heatmap from jet colors\n        jet_colors = jet(np.arange(256))[:, :3]\n        jet_heatmap = jet_colors[heatmap]\n\n        jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n        jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n        jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n        \n        superimposed_img = jet_heatmap * alpha + img\n        superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n#         plt.imshow(superimposed_img)\n        return superimposed_img\n    def forward(self,img):\n        #calling all above methods\n        img_path = img\n        img_array = preprocess_input(self.get_img_array(img_path))\n\n        \n        self.model = self.model_builder(weights=\"imagenet\")\n\n        \n        self.model.layers[-1].activation = None\n\n        preds = self.model.predict(img_array)\n\n\n        heatmap = self.make_gradcam_heatmap(img_array)\n        result = self.display_gradcam(img_path,heatmap)\n        return result\n","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:34:11.645062Z","iopub.execute_input":"2023-03-16T14:34:11.64546Z","iopub.status.idle":"2023-03-16T14:34:11.669794Z","shell.execute_reply.started":"2023-03-16T14:34:11.645417Z","shell.execute_reply":"2023-03-16T14:34:11.668942Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_Grad(model_name=\"xception\",path = None):\n    \n    '''model_name=xception,inception,mobilenetv2,vgg19,efficient'''\n    \n    #display the gradient map of PRetrained models\n    if model_name ==\"efficient\":\n        g = GradCAM(model_name=model_name,img_size = (600,600) )\n    elif model_name == \"mobilenetv2\":\n        g = GradCAM(model_name=model_name,img_size = (224,224) )\n    elif model_name == \"vgg19\":\n        g = GradCAM(model_name=model_name,img_size = (224,224) )\n    else:\n        g = GradCAM(model_name=model_name)\n    \n    \n    imgs,la = get_random_unique_sample(x,y)\n    grad_images  =[]\n    for pos,img in enumerate(imgs):\n        result = g.forward(img)\n        if path is not None:\n            cv2.imwrite(path+f\"{model_name}_{la[pos]}_{pos}.jpg\",np.array(result))\n        grad_images.append(np.array(result))\n        \n    print(\"*\"*80,model_name,\"*\"*80)\n    show_images(grad_images,titles=la,rows = 2,main_title= model_name.upper())\ndisplay_Grad(path = \"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:34:11.670967Z","iopub.execute_input":"2023-03-16T14:34:11.671333Z","iopub.status.idle":"2023-03-16T14:34:42.681536Z","shell.execute_reply.started":"2023-03-16T14:34:11.671306Z","shell.execute_reply":"2023-03-16T14:34:42.680298Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#path for saving gradcams \ndisplay_Grad(\"inception\",path = \"/kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_Grad(\"vgg19\",path = \"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:36:45.084358Z","iopub.execute_input":"2023-03-16T14:36:45.085753Z","iopub.status.idle":"2023-03-16T14:37:21.097565Z","shell.execute_reply.started":"2023-03-16T14:36:45.085704Z","shell.execute_reply":"2023-03-16T14:37:21.09664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_Grad(\"mobilenetv2\",path = \"/kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_Grad(\"efficient\",path = \"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:37:21.099272Z","iopub.execute_input":"2023-03-16T14:37:21.099653Z","iopub.status.idle":"2023-03-16T14:40:02.537835Z","shell.execute_reply.started":"2023-03-16T14:37:21.099624Z","shell.execute_reply":"2023-03-16T14:40:02.536357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = Load_Datasets(minimum_images=500,size_images = (224, 224))\nvggx,vggy,test_x,test_y = data.load_data()\n\ndata = Load_Datasets(minimum_images=500,size_images = (96, 96))\nopenfacex,openfacey,test_x,test_y = data.load_data()\n\n\ndata = Load_Datasets(minimum_images=500,size_images = (47, 55))\ndeepidx,deepidy,test_x,test_y = data.load_data()\n\n\ndata = Load_Datasets(minimum_images=500,size_images = (152, 152,))\ndeepfacex,deepfacey,test_x,test_y = data.load_data()\n\n\ndata = Load_Datasets(minimum_images=500,size_images = (160, 160))\nfacenetx,facenety,test_x,test_y = data.load_data()\n\nvgg_train_x,vgg_test_x,vgg_train_y,vgg_test_y,vgg_etol =preprocess(vggx,vggy,\"VGGFace\")\nopen_train_x,open_test_x,open_train_y,open_test_y ,open_etol=preprocess(openfacex,openfacey,\"OpenFace\")\ndeepface_train_x,deepface_test_x,deepface_train_y,deepface_test_y,deepface_etol =preprocess(deepfacex,deepfacey,\"DeepFace\")\nfacenet_train_x,facenet_test_x,facenet_train_y,facenet_test_y,facenet_etol =preprocess(facenetx,facenety,\"Facenet\")\ndeepid_train_x,deepid_test_x,deepid_train_y,deepid_test_y,deppid_etol =preprocess(deepidx,deepidy,\"DeepID\")","metadata":{"execution":{"iopub.status.busy":"2023-03-16T11:36:50.012475Z","iopub.execute_input":"2023-03-16T11:36:50.013783Z","iopub.status.idle":"2023-03-16T11:38:49.442365Z","shell.execute_reply.started":"2023-03-16T11:36:50.013734Z","shell.execute_reply":"2023-03-16T11:38:49.441046Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# class DeepFaceEmotionClassifier:\n#     def __init__(self, model_name='vggface', num_classes=None, input_shape=(96,96, 3)):\n#         print(\"Model: \",model_name.upper())\n#         self.model_name = model_name\n#         self.num_classes = num_classes\n        \n#         if self.model_name == 'vggface':\n#             self.model = DeepFace.build_model('VGG-Face')\n#         elif self.model_name == 'deepid':\n#             self.model = DeepFace.build_model('DeepID')\n#         elif self.model_name == 'deepface':\n#             self.model = DeepFace.build_model('DeepFace')\n#         elif self.model_name == 'facenet':\n#             self.model = DeepFace.build_model('Facenet')\n#         elif self.model_name == 'openface':\n#             self.model = DeepFace.build_model('OpenFace')\n#         else:\n#             raise ValueError('Invalid model_name passed - {}'.format(self.model_name))\n        \n#         for layer in self.model.layers:\n#             layer.trainable = False\n\n\n#         x = self.model.output\n#         x = Flatten()(x)\n#         x = Dense(100, activation='relu')(x)\n#         predictions = Dense(num_classes, activation='softmax', kernel_initializer='random_uniform')(x)\n\n#         self.emotion_classifier = Model(inputs=self.model.input, outputs=predictions)\n\n#         # Freezing pretrained layers\n#         self.model.trainable = False\n\n#         optimizer = Adam()\n#         self.emotion_classifier = model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n    \n#     def fit(self, X_train, y_train, X_val, y_val, batch_size=32, epochs=50):\n#         datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n#         datagen.fit(X_train)\n        \n#         hist = self.emotion_classifier.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_val, y_val), epochs=epochs)\n#         return hist,self.emotion_classifier\n    \n#     def predict(self, X):\n#         p = self.emotion_classifier.predict(X)\n#         p = np.argmax(p,axis=1)\n#         return p\n#     def evaluate(self,X,y):\n#         loss,acc = self.emotion_classifier.evaluate(X)\n        \n#         print(\"=\"*50,self.model_name,\"=\"*50)\n#         print(\"On Test Data\")\n#         print(\"Loss: \",round(loss,4))\n#         print(\"Accuracy: \",round(acc,4))\n#         return loss,acc \n\n\n\nclass DeepFaceEmotionClassifier:\n    def __init__(self,X_train, y_train,xval,yval, model_name='vggface', num_classes=len(np.unique(df))):\n        self.model_name = model_name\n        print(\"*\"*30,self.model_name.capitalize(),\"*\"*30)\n        self.num_classes = num_classes\n         \n        if self.model_name == 'vggface':\n            self.model1 = DeepFace.build_model('VGG-Face')\n        elif self.model_name == 'deepid':\n            self.model1 = DeepFace.build_model('DeepID')\n        elif self.model_name == 'deepface':\n            self.model1 = DeepFace.build_model('DeepFace')\n        elif self.model_name == 'facenet':\n            self.model1 = DeepFace.build_model('Facenet')\n        elif self.model_name == 'openface':\n            self.model1 = DeepFace.build_model('OpenFace')\n        else:\n            raise ValueError('Invalid model_name passed - {}'.format(self.model_name))\n        #FREEZING LAYERS\n        for layer in self.model1.layers[:-4]:\n            layer.trainable = False\n\n      \n        self.X_train = X_train\n        self.y_train = y_train\n        self.xval = xval\n        self.yval = yval\n        self.trainfeatures = self.model1.predict(self.X_train)\n        self.valfeatures = self.model1.predict(self.xval)\n        self.model = None\n    def fit(self,  batch_size=32, epochs=10,verbose=1):\n        # Extract features from pre-trained model\n        model = Sequential()\n        model.add(layers.Dense(20,activation= \"relu\",input_shape= (self.trainfeatures.shape[1],)))\n\n        #model.add(layers.Dense(700,activation=\"relu\"))\n        model.add(layers.Dense(len(np.unique(train_y,axis=0)),activation=\"softmax\"))\n        model.compile(optimizer = Adam(0.0001),loss = \"categorical_crossentropy\" , metrics = [\"accuracy\"])\n        history = model.fit(self.trainfeatures,self.y_train,validation_data = (self.valfeatures,self.yval),verbose=verbose,batch_size =  batch_size,epochs = epochs)    \n        self.model = model\n        return history,model\n    def predict(self, X):\n        # Extract features and make predictions\n        features = self.model1.predict(X)\n        result = self.model.predict(features)\n        result = np.argmax(result,axis=-1)\n        return result\n    def evaluate(self, X,y):\n        # Extract features and make predictions\n        features = self.model1.predict(X)\n        loss,acc = self.model.evaluate(features,y)\n        print(self.model_name)\n        print(\"On Test Data\")\n        print(\"Accuracy: \",acc)\n        print(\"Loss: \",loss)\n        return loss,acc","metadata":{"execution":{"iopub.status.busy":"2023-03-16T12:29:43.545203Z","iopub.execute_input":"2023-03-16T12:29:43.545574Z","iopub.status.idle":"2023-03-16T12:29:43.570395Z","shell.execute_reply.started":"2023-03-16T12:29:43.545522Z","shell.execute_reply":"2023-03-16T12:29:43.569149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"percent = int(len(vgg_train_x)*0.90)\nopenface =  DeepFaceEmotionClassifier(open_train_x[:percent],open_train_y[:percent],open_train_x[percent:],open_train_y[percent:],model_name = 'openface',num_classes = 8)\nopen_hist,open_model = openface.fit(batch_size = 4)\nopen_loss,open_acc = openface.evaluate(open_test_x[:percent],open_test_y[:percent])\n\ndeepid =  DeepFaceEmotionClassifier(deepid_train_x[:percent],deepid_train_y[:percent],deepid_train_x[percent:],deepid_train_y[percent:],model_name = 'deepid',num_classes = 8)\ndeepid_hist,deepid_model = deepid.fit(batch_size = 4)\ndeepid_loss,deepid_acc = deepid.evaluate(deepid_test_x[:percent],deepid_test_y[:percent])\n\n\ndeepface =  DeepFaceEmotionClassifier(deepface_train_x[:percent],deepface_train_y[:percent],deepface_train_x[percent:],deepface_train_y[percent:],model_name = 'deepface',num_classes = 8)\ndeepface_hist,deepface_model = deepface.fit(batch_size = 4)\ndeepface_loss,deepface_acc = deepface.evaluate(deepface_test_x[:percent],deepface_test_y[:percent])\n\n\nfacenet = DeepFaceEmotionClassifier(facenet_train_x[:percent],facenet_train_y[:percent],facenet_train_x[percent:],facenet_train_y[percent:],model_name = 'facenet',num_classes = 8)\nfacenet_hist,facenet_model = facenet.fit(batch_size = 4)\nfacenet_loss,facenet_acc = facenet.evaluate(facenet_test_x[:percent],facenet_test_y[:percent])\n\n\nvgg = DeepFaceEmotionClassifier(vgg_train_x[:percent],vgg_train_y[:percent],vgg_train_x[percent:],vgg_train_y[percent:],model_name = 'vggface',num_classes = 8)\nvgg_hist,vgg_model = vgg.fit(batch_size = 4)\nvgg_loss,vgg_acc = vgg.evaluate(vgg_test_x[:percent],vgg_test_y[:percent])","metadata":{"execution":{"iopub.status.busy":"2023-03-16T12:29:44.015684Z","iopub.execute_input":"2023-03-16T12:29:44.01639Z","iopub.status.idle":"2023-03-16T12:31:34.177064Z","shell.execute_reply.started":"2023-03-16T12:29:44.016356Z","shell.execute_reply":"2023-03-16T12:31:34.176041Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-03-16T11:49:46.307509Z","iopub.execute_input":"2023-03-16T11:49:46.308323Z","iopub.status.idle":"2023-03-16T11:49:46.313375Z","shell.execute_reply.started":"2023-03-16T11:49:46.308286Z","shell.execute_reply":"2023-03-16T11:49:46.312458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}